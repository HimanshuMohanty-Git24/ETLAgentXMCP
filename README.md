# ETL Agentic X MCP - Intelligent Medallion Pipeline

**An AI-driven, multi-agent ETL orchestration system using LangGraph and Databricks Claude Sonnet 4.5**

Transform raw data through Bronze â†’ Silver â†’ Gold layers with intelligent agents that plan, code, review, and execute transformations automatically. Each layer is backed by a GitHub PR with context-aware code generation.

---

## ğŸ¯ Quick Start

### Prerequisites
- Python 3.12+
- Databricks workspace with serverless SQL warehouse
- GitHub repository with PAT token
- Databricks personal access token

### Installation

```bash
# Clone and install
git clone <your-repo>
cd ETLAgenticXMCP

# Configure environment
cp .env.example .env
# Edit .env with your credentials

# Install dependencies
pip install -r requirements.txt
```

### Run the Pipeline

```python
from graph_workflow import MEDALLION_PIPELINE

# Execute full Bronze â†’ Silver â†’ Gold transformation
result = MEDALLION_PIPELINE.invoke({
    "user_query": "Clean weather data and create daily city aggregations",
    "source_table": "samples.accuweather.forecast_daily_calendar_imperial"
})
```

Or use the MCP server for VS Code integration:

```bash
python server.py
```

---

## ğŸ—ï¸ Architecture Overview

```mermaid
flowchart TB
    subgraph user_interface["User Interface"]
        vscode_client["VS Code + Copilot"]
        prompt_entry["User Prompt Entry"]
    end

    subgraph mcp_layer["MCP Protocol & Orchestration"]
        mcp_server["FastMCP Server (Python)"]
        langgraph_orch["LangGraph Workflow"]
        etl_state["ETL State Management"]
    end

    subgraph agent_layer["Agent Layer (Databricks Claude 4.5)"]
        planner_agent["PlannerAgent (Claude)"]
        codegen_agent["CodeGenAgent (Claude)"]
        reviewer_agent["ReviewerAgent (Claude)"]
        pr_creator_agent["PRCreatorAgent (GitHub API)"]
        executor_agent["ExecutorAgent (Databricks SDK)"]
        context_agent["ContextEnrichmentAgent"]
        summary_agent["SummaryAgent (Claude)"]
    end

    subgraph tools_integration["Python Integration & External APIs"]
        databricks_tools["Databricks Tools (SDK, SQL)"]
        github_tools["GitHub Tools (REST, GitPython)"]
        medallion_tools["Medallion Utils (SQL Generation)"]
        context_tools["Context Tools (Schema, Analysis)"]
    end

    subgraph databricks_infra["Databricks Workspace"]
        workspace["Workspace / Serverless Infra"]
        foundation_model_api["Foundation Model API<br/>Claude Sonnet 4.5 Endpoint"]
        sql_warehouse["SQL Warehouse (Serverless)"]
        delta_tables["Delta Lake Tables (Bronze/Silver/Gold)"]
        unity_catalog["Unity Catalog"]
    end

    subgraph github_vcs["GitHub Version Control"]
        github_repo["Repository"]
        github_pr["Pull Requests"]
        github_ci["Security & Approval"]
    end

    subgraph storage["Medallion Data Layers"]
        bronze_table["Bronze Table (Raw)"]
        silver_table["Silver Table (Clean)"]
        gold_table["Gold Table (Aggregate)"]
    end

    subgraph config["Configuration"]
        env_vars[".env File"]
        rules_txt["rules.txt"]
    end

    %% USER TO INTERFACE
    vscode_client --> prompt_entry
    prompt_entry --> mcp_server

    %% CONFIGURATION
    env_vars --> mcp_server
    rules_txt --> mcp_server

    %% MCP to ORCHESTRATION
    mcp_server --> langgraph_orch
    mcp_server --> etl_state
    etl_state <--> langgraph_orch

    %% Orchestrator calls agents in sequence
    langgraph_orch --> planner_agent
    planner_agent --> codegen_agent
    codegen_agent --> reviewer_agent
    reviewer_agent -.-> pr_creator_agent
    pr_creator_agent --> executor_agent
    executor_agent --> context_agent
    context_agent -.-> planner_agent
    context_agent --> summary_agent
    summary_agent --> mcp_server

    %% AGENTS to TOOLS
    planner_agent --> databricks_tools
    codegen_agent --> medallion_tools
    reviewer_agent --> databricks_tools
    pr_creator_agent --> github_tools
    executor_agent --> databricks_tools
    context_agent --> context_tools

    %% DATABRICKS: Model API and SQL Warehouse
    planner_agent --> foundation_model_api
    codegen_agent --> foundation_model_api
    reviewer_agent --> foundation_model_api
    summary_agent --> foundation_model_api

    executor_agent --> sql_warehouse
    databricks_tools --> sql_warehouse
    medallion_tools --> sql_warehouse
    context_tools --> sql_warehouse
    sql_warehouse --> delta_tables
    delta_tables --> unity_catalog

    %% DATA FLOW Bronze â†’ Silver â†’ Gold
    bronze_table --> silver_table
    silver_table --> gold_table

    %% Delta tables backed by physical tables
    bronze_table --> delta_tables
    silver_table --> delta_tables
    gold_table --> delta_tables

    %% CONTEXT ENRICHMENT
    bronze_table -.-> context_agent
    silver_table -.-> context_agent
    gold_table -.-> context_agent

    %% PR Creator â†’ GitHub
    pr_creator_agent --> github_pr
    github_pr --> github_repo
    github_repo --> github_ci
    github_pr -.-> executor_agent

    %% Final summary returns to user
    summary_agent --> vscode_client
```

---

## ğŸ“Š Pipeline Workflow

### Single Layer Transformation

For each medallion layer (Bronze, Silver, Gold):

```
1. ğŸ¯ PLANNER
   â”œâ”€ Analyzes user query & transformation rules
   â”œâ”€ Reviews context from previous layer (if available)
   â””â”€ Creates detailed transformation plan

2. ğŸ’» CODE GENERATOR
   â”œâ”€ Generates PySpark/SQL code
   â”œâ”€ Includes error handling & data quality checks
   â””â”€ Follows medallion best practices

3. ğŸ” REVIEWER
   â”œâ”€ Validates syntax & logic
   â”œâ”€ Checks against rules.txt compliance
   â”œâ”€ Generates quality score
   â””â”€ Approves or requests revision

4. ğŸ”— PR CREATOR (if approved)
   â”œâ”€ Creates GitHub PR with generated code
   â”œâ”€ Adds documentation & test plans
   â””â”€ Waits for approval & merge

5. âš¡ EXECUTOR (on PR merge)
   â”œâ”€ Detects PR merge event
   â”œâ”€ Executes transformation on Databricks
   â””â”€ Monitors execution & collects metrics

6. ğŸ“ˆ CONTEXT ENRICHMENT
   â”œâ”€ Queries output table schema
   â”œâ”€ Analyzes data quality metrics
   â”œâ”€ Creates summary for next layer
   â””â”€ Loops back to PLANNER for next layer

7. ğŸ“ SUMMARY GENERATOR (all layers complete)
   â”œâ”€ Aggregates execution metrics
   â”œâ”€ Generates executive report
   â””â”€ Returns to user
```

### Multi-Layer Intelligence

Each layer receives **real, fresh context** from the previous layer:

```
Bronze Layer
   â†“ [Context: Raw data schema, row counts, data types]
Silver Layer (uses Bronze context)
   â†“ [Context: Cleaned data schema, quality metrics, duplicates removed]
Gold Layer (uses Silver context)
   â†“
Executive Summary
```

---

## ğŸ”§ Core Components

### Agents (`agents/`)

| Agent | Role |
|-------|------|
| **PlannerAgent** | Analyzes requests, creates transformation strategy, enriches from prior context |
| **CodeGenAgent** | Generates PySpark/SQL code with medallion patterns, validation, and error handling |
| **ReviewerAgent** | Validates code quality, syntax, compliance with rules, generates confidence score |
| **PRCreatorAgent** | Creates GitHub PRs with code, documentation, test plans, tracks merge status |
| **ExecutorAgent** | Executes code on Databricks after PR merge, monitors jobs, collects metrics |
| **ContextEnrichmentAgent** | Analyzes layer output, extracts schema/metrics, prepares context for next layer |
| **SummaryAgent** | Generates executive summary with PR links, metrics, and business insights |

### Tools (`tools/`)

| Module | Purpose |
|--------|---------|
| **databricks_tools.py** | Databricks SDK wrapper - SQL execution, warehouse queries, job monitoring |
| **github_tools.py** | GitHub API & GitPython - PR creation, merge detection, repo operations |
| **medallion_tools.py** | SQL template generators - Bronze/Silver/Gold patterns with transformations |
| **context_tools.py** | Schema extraction, data quality analysis, metrics aggregation |

### State Management (`state.py`)

Centralized ETL state tracking:
- Layer progress (completed/remaining)
- PR history with merge status
- Data quality metrics
- Error logging
- Context accumulation across layers

### Orchestration (`graph_workflow.py`)

LangGraph workflow with:
- Conditional routing based on approval status
- Sequential multi-layer processing
- Context enrichment between layers
- Error handling and fallback paths

### MCP Server (`server.py`)

FastMCP integration exposing:
- `run_full_medallion_pipeline()` - Execute complete Bronzeâ†’Silverâ†’Gold
- `check_transformation_status()` - Monitor PR/execution status
- `view_transformation_rules()` - Display/manage transformation rules
- Event-driven PR merge detection

---

## ğŸ“‹ Transformation Rules

Rules are defined in `rules.txt` and applied across all layers:

### Bronze Layer Rules
- âœ… Ingest raw data as-is
- âœ… Add audit metadata (ingestion_timestamp, source_file, row_id)
- âœ… Store in Delta Lake with Change Data Feed
- âœ… No transformations applied

### Silver Layer Rules
- ğŸ§¹ Remove duplicates
- ğŸ”¤ Standardize data types & text case
- âŒ Remove null critical values
- ğŸ“Š Add quality flags (is_valid_record, data_quality_score)
- ğŸš« Filter records with score < 0.7
- ğŸ“… Partition by date

### Gold Layer Rules
- ğŸ“ˆ Aggregate by city & country
- ğŸ§® Calculate derived metrics (comfort_index, etc.)
- ğŸ—‚ï¸ Create dimension & fact tables
- ğŸ” Z-order cluster on filtered columns
- ğŸ“Š Generate trend views (weekly, monthly)

### Data Quality Checks
- Temperature: -50Â°C to 60Â°C
- Humidity: 0-100%
- Coordinates: Latitude Â±90, Longitude Â±180
- Wind/precipitation: â‰¥ 0
- Outliers: Flag beyond 3Ïƒ

---

## ğŸš€ Key Features

### ğŸ¤– AI-Driven Decision Making
- Claude Sonnet 4.5 powers all agents
- Natural language transformation requests
- Intelligent code generation with error handling
- Adaptive context enrichment

### ğŸ”„ Context-Aware Pipeline
- Each layer reads output from previous layer
- Fresh schema & metrics inform next planning phase
- No hardcoded assumptions - truly adaptive

### ğŸ”— GitHub-Native Workflow
- One PR per layer transformation
- Automatic merge detection triggers execution
- Full audit trail in Git history
- Security reviews built-in

### ğŸ“Š Built-in Quality Assurance
- Automated code review before PR
- Data quality metrics tracked per layer
- Quality score thresholds enforced
- Comprehensive error logging

### âš¡ Serverless & Scalable
- Databricks serverless SQL warehouse
- Auto-scaling executors
- Delta Lake Change Data Feed
- Unity Catalog for governance

### ğŸ¯ Enterprise-Ready
- Multi-layer medallion architecture
- Idempotent transformations
- Comprehensive logging & monitoring
- Compliance-ready data lineage

---

## ğŸ“¦ Environment Setup

Create `.env` file:

```bash
# Databricks
DATABRICKS_HOST=https://your-workspace.databricks.com
DATABRICKS_TOKEN=dapi2xxxxx
DATABRICKS_WAREHOUSE_ID=xxxxx
DATABRICKS_MODEL_ENDPOINT=databricks-claude-sonnet-4-5

# Model Configuration
MODEL_TEMPERATURE=0.2
MODEL_MAX_TOKENS=4096
MODEL_TOP_P=0.95

# GitHub
GITHUB_TOKEN=ghp_xxxxx
GITHUB_REPO_OWNER=your-username
GITHUB_REPO_NAME=your-repo
GIT_LOCAL_PATH=/path/to/local/repo

# Databricks Defaults
DEFAULT_CATALOG=samples
DEFAULT_SCHEMA=accuweather

# Logging
LOG_LEVEL=INFO
DEBUG_API_CALLS=false
```

---

## ğŸ’» Usage Examples

### Example 1: Full Pipeline Execution

```python
from graph_workflow import MEDALLION_PIPELINE

state = MEDALLION_PIPELINE.invoke({
    "user_query": "Clean weather data and create daily city aggregations",
    "source_table": "samples.accuweather.forecast_daily_calendar_imperial"
})

print(state["executive_summary"])
```

### Example 2: Custom Transformation Rules

Edit `rules.txt` then run:

```bash
python server.py
# Access via VS Code MCP tools
```

### Example 3: Monitor Transformation Status

```python
from server import check_transformation_status

status = await check_transformation_status(pr_number=42)
print(status)
```

### Example 4: View Transformation Rules

```python
from server import view_transformation_rules

rules = await view_transformation_rules()
print(rules)
```

---

## ğŸ“ˆ Pipeline Execution Flow

```
User Query
    â†“
[MCP Server receives request]
    â†“
[Initialize ETLState]
    â†“
â”Œâ”€ BRONZE LAYER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Planner â†’ CodeGen â†’ Reviewer â†’ PR â†’ Executor  â”‚
â”‚ [Create bronze_table from source]             â”‚
â”‚ [Enrich context with schema/metrics]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ [Context: Bronze table schema, row_count]
â”Œâ”€ SILVER LAYER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Planner (with Bronze context) â†’ ... â†’ Executorâ”‚
â”‚ [Clean & deduplicate bronze_table]            â”‚
â”‚ [Create silver_table with quality flags]      â”‚
â”‚ [Enrich context with quality metrics]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ [Context: Silver table schema, quality score]
â”Œâ”€ GOLD LAYER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Planner (with Silver context) â†’ ... â†’ Executorâ”‚
â”‚ [Aggregate silver_table by dimensions]        â”‚
â”‚ [Create gold_table with derived metrics]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Generate Executive Summary]
    â†“
Return to User with:
  - 3 PR links (Bronze, Silver, Gold)
  - Execution metrics & timing
  - Data quality scores
  - Error logs (if any)
  - Next steps & recommendations
```

---

## ğŸ” Monitoring & Debugging

### Enable Debug Logging

```bash
# In .env
LOG_LEVEL=DEBUG
DEBUG_API_CALLS=true
```

### Check Execution Logs

```bash
# View server logs
tail -f medallion_etl.log

# Check Git PR history
git log --oneline | grep "medallion\|etl"
```

### Monitor Databricks Jobs

- Open Databricks workspace
- Navigate to Workflows â†’ view executed notebooks
- Check job runs for performance metrics

---

## ğŸ“š File Structure

```
ETLAgenticXMCP/
â”œâ”€â”€ agents/                      # AI-powered agents
â”‚   â”œâ”€â”€ planner_agent.py         # Transformation planning
â”‚   â”œâ”€â”€ codegen_agent.py         # SQL/PySpark generation
â”‚   â”œâ”€â”€ reviewer_agent.py        # Code quality review
â”‚   â”œâ”€â”€ pr_creator_agent.py      # GitHub PR management
â”‚   â”œâ”€â”€ executor_agent.py        # Databricks execution
â”‚   â”œâ”€â”€ context_enrichment_agent.py  # Context extraction
â”‚   â””â”€â”€ summary_agent.py         # Report generation
â”œâ”€â”€ tools/                       # External integrations
â”‚   â”œâ”€â”€ databricks_tools.py      # Databricks SDK wrapper
â”‚   â”œâ”€â”€ github_tools.py          # GitHub API & GitPython
â”‚   â”œâ”€â”€ medallion_tools.py       # SQL generators
â”‚   â””â”€â”€ context_tools.py         # Schema & metrics
â”œâ”€â”€ graph_workflow.py            # LangGraph orchestration
â”œâ”€â”€ state.py                     # ETL state definition
â”œâ”€â”€ server.py                    # FastMCP server
â”œâ”€â”€ main.py                      # CLI entry point
â”œâ”€â”€ rules.txt                    # Transformation rules
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ .env.example                 # Environment template
```

---

## ğŸ› ï¸ Dependencies

- **langgraph** - Multi-agent orchestration
- **langchain** - LLM framework
- **databricks-langchain** - Databricks integration
- **databricks-sdk** - Databricks API client
- **fastmcp** - MCP protocol server
- **gitpython** - Git operations
- **requests** - HTTP client
- **pydantic** - Data validation

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/your-feature`
3. Commit changes: `git commit -am 'Add feature'`
4. Push to branch: `git push origin feature/your-feature`
5. Submit pull request

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ“ Support

For issues, questions, or feature requests:
- Open a GitHub issue
- Check existing documentation in `rules.txt`
- Review agent logs for debugging

---

**Built with â¤ï¸ by the Data Engineering Team**  
*Powered by Databricks Claude Sonnet 4.5 & LangGraph*